{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Meme Understanding with CLIP and BLIP-2\n", "\n", "This notebook demonstrates a multimodal analysis of internet memes using two powerful vision-language models:\n", "\n", "- **CLIP (Contrastive Language-Image Pre-training)**: Identifies the most probable text description (caption) for a given image.\n", "- **BLIP-2 (Bootstrapped Language-Image Pretraining)**: Generates captions and explanations based on image content.\n", "\n", "We aim to:\n", "1. Evaluate CLIP's ability to select the correct caption from multiple options.\n", "2. Generate free-form meme captions using BLIP-2.\n", "3. Generate reasoning for humor detection using BLIP-2."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 1. Imports and Setup\n", "import os\n", "import torch\n", "from PIL import Image\n", "import json\n", "import clip\n", "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n", "import matplotlib.pyplot as plt\n", "\n", "# Check device (GPU if available)\n", "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n", "print(\"Using device:\", device)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Load Pretrained Models\n", "\n", "We load:\n", "- `CLIP` (ViT-B/32) model and its preprocessing pipeline.\n", "- `BLIP-2` (Flan-T5-XL) with both processor and model for captioning and reasoning."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Load CLIP\n", "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n", "\n", "# Load BLIP-2\n", "blip_processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\n", "blip_model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-flan-t5-xl\").to(device)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. Load Meme Dataset\n", "\n", "Each meme in the dataset contains:\n", "- `image_filename`\n", "- a list of `captions`\n", "- `correct_caption_index`\n", "\n", "Images are stored in the local `./memes` folder."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["with open(\"real_meme_dataset_clip_blip.json\", \"r\") as f:\n", "    meme_data = json.load(f)\n", "\n", "image_folder = \"./memes\"\n", "results = []"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. Process Each Meme\n", "\n", "For each meme:\n", "- Use CLIP to match image with best caption from given options.\n", "- Use BLIP-2 to:\n", "  - Generate a caption without prompt\n", "  - Generate a caption with the prompt: *\"Caption this meme\"*\n", "  - Explain the humor with: *\"Explain why it is funny.\"*"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for meme in meme_data:\n", "    image_path = os.path.join(image_folder, meme[\"image_filename\"])\n", "    if not os.path.exists(image_path):\n", "        print(f\"Image not found: {image_path}\")\n", "        continue\n", "\n", "    raw_image = Image.open(image_path).convert(\"RGB\")\n", "    image_input_clip = clip_preprocess(raw_image).unsqueeze(0).to(device)\n", "    text_inputs = clip.tokenize(meme[\"captions\"]).to(device)\n", "\n", "    with torch.no_grad():\n", "        logits_per_image, _ = clip_model(image_input_clip, text_inputs)\n", "        probs = logits_per_image.softmax(dim=-1).cpu().numpy()[0]\n", "\n", "    predicted_index = int(probs.argmax())\n", "    correct = (predicted_index == meme[\"correct_caption_index\"])\n", "\n", "    blip_inputs = blip_processor(raw_image, return_tensors=\"pt\").to(device)\n", "    with torch.no_grad():\n", "        out = blip_model.generate(**blip_inputs)\n", "    blip_caption = blip_processor.decode(out[0], skip_special_tokens=True)\n", "\n", "    prompt = f\"Caption this meme\"\n", "    blip_inputs_prompt = blip_processor(raw_image, prompt, return_tensors=\"pt\").to(device)\n", "    with torch.no_grad():\n", "        out_prompt = blip_model.generate(**blip_inputs_prompt, max_new_tokens=100)\n", "    blip_caption_with_prompt = blip_processor.tokenizer.decode(out_prompt[0], skip_special_tokens=True)\n", "\n", "    reason = f\"Caption of the meme is: {blip_caption_with_prompt}. Explain why it is funny.\"\n", "    blip_inputs_reason = blip_processor(raw_image, reason, return_tensors=\"pt\").to(device)\n", "    with torch.no_grad():\n", "        out_reason = blip_model.generate(**blip_inputs_reason, max_new_tokens=100)\n", "    blip_reason = blip_processor.tokenizer.decode(out_reason[0], skip_special_tokens=True)\n", "\n", "    results.append({\n", "        \"meme_id\": meme[\"id\"],\n", "        \"clip_prediction_index\": predicted_index,\n", "        \"clip_predicted_caption\": meme[\"captions\"][predicted_index],\n", "        \"clip_correct\": correct,\n", "        \"clip_probs\": [float(p) for p in probs],\n", "        \"blip_caption\": blip_caption,\n", "        \"blip_caption_with_prompt\": blip_caption_with_prompt,\n", "        \"blip_reason\": blip_reason,\n", "        \"correct_caption\": meme[\"correct_caption\"]\n", "    })"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5. Accuracy & Results Preview\n", "\n", "Check CLIP's top-1 accuracy and preview predictions and generations from both models."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["clip_accuracy = sum([r[\"clip_correct\"] for r in results]) / len(results)\n", "print(f\"\\nCLIP Accuracy: {clip_accuracy:.2%}\\n\")\n", "\n", "for r in results[:5]:\n", "    print(f\"Meme ID: {r['meme_id']}\")\n", "    print(f\"CLIP Prediction: {r['clip_predicted_caption']}\")\n", "    print(f\"BLIP Caption (Image Only): {r['blip_caption']}\")\n", "    print(f\"BLIP Caption (With Prompt): {r['blip_caption_with_prompt']}\")\n", "    print(f\"BLIP Reasoning: {r['blip_reason']}\")\n", "    print(f\"Correct Caption: {r['correct_caption']}\")\n", "    print(\"--\" * 30)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 6. Save Results\n", "\n", "Save the complete result analysis as a JSON file for further analysis or visualization."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["with open(\"meme_analysis_results.json\", \"w\") as out_file:\n", "    json.dump(results, out_file, indent=2)\n", "\n", "print(\"Analysis complete. Results saved to meme_analysis_results.json\")"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 2}